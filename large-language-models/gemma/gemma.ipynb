{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral vs LLama 2 vs Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/nous-hermes-llama-2-7b.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/gemma-7b-it-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   28 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_rot            = 192\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.77 GiB (4.80 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4883.94 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.13 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     7.91 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '15', 'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'gemma', 'gemma.attention.head_count_kv': '16', 'gemma.feed_forward_length': '24576', 'tokenizer.ggml.bos_token_id': '2', 'gemma.embedding_length': '3072', 'gemma.block_count': '28', 'tokenizer.ggml.unknown_token_id': '3', 'gemma.attention.key_length': '256', 'gemma.context_length': '8192', 'general.name': 'gemma-7b-it', 'gemma.attention.value_length': '256', 'gemma.attention.head_count': '16'}\n",
      "Using fallback chat format: None\n",
      "/Users/rafael/miniconda3/envs/zaai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/rafael/miniconda3/envs/zaai/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from encoder.encoder import Encoder\n",
    "from retriever.vector_db import VectorDatabase\n",
    "from langchain.docstore.document import Document\n",
    "from generator.generator import Generator\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "llama = Generator(model='llama')\n",
    "mistral = Generator(model='mistral')\n",
    "gemma = Generator(model='gemma')\n",
    "encoder = Encoder()\n",
    "vectordb = VectorDatabase(encoder.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and insert into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n",
      "Collection not found\n"
     ]
    }
   ],
   "source": [
    "df[\"full_review\"] = df[[\"reviews.title\", \"reviews.text\"]].apply(\n",
    "    lambda row: \". \".join(row.values.astype(str)), axis=1\n",
    ")\n",
    "for product_id in df[\"asins\"].unique()[:10]:\n",
    "    # create documents to store in Postgres\n",
    "    docs = [\n",
    "        Document(page_content=item)\n",
    "        for item in df[df[\"asins\"] == product_id][\"full_review\"].tolist()\n",
    "    ]\n",
    "\n",
    "    passages = vectordb.create_passages_from_documents(docs)\n",
    "    vectordb.store_passages_db(passages, product_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate queries and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 questions for each product id (20 questions in total)\n",
    "questions = [f\"{product_id}|What people like about the product?\" for product_id in df[\"asins\"].unique()[:10]] + [f\"{product_id}|What people dislike about the product?\" for product_id in df[\"asins\"].unique()[:10]]\n",
    "\n",
    "# retrieve query and context to give to llama and mistral\n",
    "QUERIES = []\n",
    "CONTEXTS = []\n",
    "\n",
    "for q in questions:\n",
    "    id = q.split(\"|\")[0]\n",
    "    query = q.split(\"|\")[1]\n",
    "    context = vectordb.retrieve_most_similar_document(query, k=2, id=id)\n",
    "    QUERIES.append(query)\n",
    "    CONTEXTS.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary to save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": []\n",
    "}\n",
    "\n",
    "mistral_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": []\n",
    "}\n",
    "\n",
    "gemma_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafael/miniconda3/envs/zaai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      39.86 ms /   109 runs   (    0.37 ms per token,  2734.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43763.65 ms /   322 tokens (  135.91 ms per token,     7.36 tokens per second)\n",
      "llama_print_timings:        eval time =   24875.99 ms /   108 runs   (  230.33 ms per token,     4.34 tokens per second)\n",
      "llama_print_timings:       total time =   69128.61 ms /   430 tokens\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      30.43 ms /    86 runs   (    0.35 ms per token,  2826.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44685.77 ms /   319 tokens (  140.08 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time =   20028.24 ms /    85 runs   (  235.63 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:       total time =   65127.53 ms /   404 tokens\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     159.57 ms /    78 runs   (    2.05 ms per token,   488.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   53934.75 ms /   296 tokens (  182.21 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:        eval time =   21900.51 ms /    77 runs   (  284.42 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:       total time =   78457.90 ms /   373 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      10.65 ms /    31 runs   (    0.34 ms per token,  2910.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21631.05 ms /   100 tokens (  216.31 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time =    6531.46 ms /    30 runs   (  217.72 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =   28309.54 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      18.65 ms /    57 runs   (    0.33 ms per token,  3055.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21567.64 ms /    98 tokens (  220.08 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:        eval time =   12658.01 ms /    56 runs   (  226.04 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:       total time =   34463.42 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     147.33 ms /    54 runs   (    2.73 ms per token,   366.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26461.94 ms /    96 tokens (  275.65 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:        eval time =   14910.29 ms /    54 runs   (  276.12 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   43002.12 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      18.03 ms /    68 runs   (    0.27 ms per token,  3772.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88225.70 ms /   739 tokens (  119.39 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:        eval time =   15672.03 ms /    67 runs   (  233.91 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =  104374.47 ms /   806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      41.97 ms /   157 runs   (    0.27 ms per token,  3741.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90045.30 ms /   722 tokens (  124.72 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:        eval time =   37725.46 ms /   156 runs   (  241.83 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =  128574.64 ms /   878 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     134.87 ms /    65 runs   (    2.07 ms per token,   481.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  102371.49 ms /   677 tokens (  151.21 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:        eval time =   18599.82 ms /    64 runs   (  290.62 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =  124110.67 ms /   741 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      11.61 ms /    38 runs   (    0.31 ms per token,  3273.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27416.07 ms /   162 tokens (  169.24 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:        eval time =    8328.03 ms /    37 runs   (  225.08 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   35942.58 ms /   199 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    20 runs   (    0.32 ms per token,  3082.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28659.94 ms /   162 tokens (  176.91 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:        eval time =    4219.49 ms /    19 runs   (  222.08 ms per token,     4.50 tokens per second)\n",
      "llama_print_timings:       total time =   33015.28 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     150.55 ms /    61 runs   (    2.47 ms per token,   405.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33636.11 ms /   154 tokens (  218.42 ms per token,     4.58 tokens per second)\n",
      "llama_print_timings:        eval time =   16561.56 ms /    60 runs   (  276.03 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   52045.22 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      30.33 ms /   109 runs   (    0.28 ms per token,  3593.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   94541.88 ms /   794 tokens (  119.07 ms per token,     8.40 tokens per second)\n",
      "llama_print_timings:        eval time =   27744.49 ms /   108 runs   (  256.89 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =  122965.53 ms /   902 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      22.82 ms /    76 runs   (    0.30 ms per token,  3329.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101619.68 ms /   772 tokens (  131.63 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time =   17874.00 ms /    75 runs   (  238.32 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:       total time =  120048.80 ms /   847 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     239.75 ms /   110 runs   (    2.18 ms per token,   458.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  108544.26 ms /   724 tokens (  149.92 ms per token,     6.67 tokens per second)\n",
      "llama_print_timings:        eval time =   31920.70 ms /   109 runs   (  292.85 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =  144900.62 ms /   833 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    19 runs   (    0.29 ms per token,  3420.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  116285.46 ms /   984 tokens (  118.18 ms per token,     8.46 tokens per second)\n",
      "llama_print_timings:        eval time =    4385.39 ms /    18 runs   (  243.63 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:       total time =  121089.53 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      14.81 ms /    50 runs   (    0.30 ms per token,  3376.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  117929.62 ms /   952 tokens (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:        eval time =   12276.93 ms /    50 runs   (  245.54 ms per token,     4.07 tokens per second)\n",
      "llama_print_timings:       total time =  130740.75 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =      68.88 ms /    31 runs   (    2.22 ms per token,   450.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  132916.43 ms /   884 tokens (  150.36 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time =    8781.36 ms /    30 runs   (  292.71 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =  144692.42 ms /   914 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      49.17 ms /   175 runs   (    0.28 ms per token,  3558.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43880.45 ms /   320 tokens (  137.13 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:        eval time =   42006.24 ms /   175 runs   (  240.04 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:       total time =   86711.93 ms /   495 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      20.55 ms /    74 runs   (    0.28 ms per token,  3600.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49466.02 ms /   318 tokens (  155.55 ms per token,     6.43 tokens per second)\n",
      "llama_print_timings:        eval time =   17145.75 ms /    73 runs   (  234.87 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =   67033.04 ms /   391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     301.48 ms /   148 runs   (    2.04 ms per token,   490.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54343.63 ms /   304 tokens (  178.76 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:        eval time =   42380.94 ms /   147 runs   (  288.31 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =  101199.62 ms /   451 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      36.69 ms /   102 runs   (    0.36 ms per token,  2780.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72275.62 ms /   575 tokens (  125.70 ms per token,     7.96 tokens per second)\n",
      "llama_print_timings:        eval time =   23432.45 ms /   101 runs   (  232.00 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:       total time =   96319.24 ms /   676 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      34.21 ms /    94 runs   (    0.36 ms per token,  2747.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74119.59 ms /   556 tokens (  133.31 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time =   22353.28 ms /    93 runs   (  240.36 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:       total time =   97070.21 ms /   649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     235.05 ms /    85 runs   (    2.77 ms per token,   361.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   87708.17 ms /   538 tokens (  163.03 ms per token,     6.13 tokens per second)\n",
      "llama_print_timings:        eval time =   23976.40 ms /    84 runs   (  285.43 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =  115435.61 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      21.47 ms /    80 runs   (    0.27 ms per token,  3725.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  102947.21 ms /   852 tokens (  120.83 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:        eval time =   18880.14 ms /    79 runs   (  238.99 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =  122487.96 ms /   931 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      29.01 ms /   101 runs   (    0.29 ms per token,  3481.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  107076.64 ms /   826 tokens (  129.63 ms per token,     7.71 tokens per second)\n",
      "llama_print_timings:        eval time =   24683.45 ms /   100 runs   (  246.83 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:       total time =  132530.41 ms /   926 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     174.03 ms /    82 runs   (    2.12 ms per token,   471.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  119130.05 ms /   766 tokens (  155.52 ms per token,     6.43 tokens per second)\n",
      "llama_print_timings:        eval time =   23525.71 ms /    81 runs   (  290.44 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =  146923.42 ms /   847 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      40.97 ms /   116 runs   (    0.35 ms per token,  2831.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  108088.81 ms /   887 tokens (  121.86 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time =   27626.48 ms /   115 runs   (  240.23 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:       total time =  136504.86 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      48.20 ms /   140 runs   (    0.34 ms per token,  2904.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  108336.43 ms /   846 tokens (  128.06 ms per token,     7.81 tokens per second)\n",
      "llama_print_timings:        eval time =   34182.88 ms /   139 runs   (  245.92 ms per token,     4.07 tokens per second)\n",
      "llama_print_timings:       total time =  143394.06 ms /   985 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     312.72 ms /   111 runs   (    2.82 ms per token,   354.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  125230.32 ms /   790 tokens (  158.52 ms per token,     6.31 tokens per second)\n",
      "llama_print_timings:        eval time =   33489.10 ms /   110 runs   (  304.45 ms per token,     3.28 tokens per second)\n",
      "llama_print_timings:       total time =  163625.01 ms /   900 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      11.42 ms /    41 runs   (    0.28 ms per token,  3591.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90777.06 ms /   736 tokens (  123.34 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9367.59 ms /    40 runs   (  234.19 ms per token,     4.27 tokens per second)\n",
      "llama_print_timings:       total time =  100589.64 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      26.29 ms /    95 runs   (    0.28 ms per token,  3613.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   95804.66 ms /   726 tokens (  131.96 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:        eval time =   23836.23 ms /    94 runs   (  253.58 ms per token,     3.94 tokens per second)\n",
      "llama_print_timings:       total time =  120323.79 ms /   820 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     146.71 ms /    68 runs   (    2.16 ms per token,   463.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  109197.09 ms /   678 tokens (  161.06 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:        eval time =   19243.36 ms /    67 runs   (  287.21 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:       total time =  132037.91 ms /   745 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      28.98 ms /    78 runs   (    0.37 ms per token,  2691.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21589.54 ms /   103 tokens (  209.61 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:        eval time =   17267.31 ms /    77 runs   (  224.25 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time =   39270.55 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    17 runs   (    0.38 ms per token,  2651.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21998.72 ms /   101 tokens (  217.81 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4211.46 ms /    16 runs   (  263.22 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:       total time =   26360.47 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =      88.65 ms /    28 runs   (    3.17 ms per token,   315.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27743.13 ms /    99 tokens (  280.23 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =    7722.41 ms /    27 runs   (  286.02 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =   36675.53 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      17.03 ms /    60 runs   (    0.28 ms per token,  3522.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90701.14 ms /   740 tokens (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time =   13864.88 ms /    59 runs   (  235.00 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =  105104.89 ms /   799 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      24.10 ms /    86 runs   (    0.28 ms per token,  3568.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   95613.79 ms /   723 tokens (  132.25 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time =   20067.57 ms /    85 runs   (  236.09 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:       total time =  116290.06 ms /   808 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     100.37 ms /    46 runs   (    2.18 ms per token,   458.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  105436.34 ms /   677 tokens (  155.74 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:        eval time =   12847.73 ms /    45 runs   (  285.51 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =  121160.17 ms /   722 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =       8.14 ms /    24 runs   (    0.34 ms per token,  2949.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29002.15 ms /   163 tokens (  177.93 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:        eval time =    5327.38 ms /    23 runs   (  231.63 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:       total time =   34510.21 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =       6.78 ms /    20 runs   (    0.34 ms per token,  2950.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30728.92 ms /   163 tokens (  188.52 ms per token,     5.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.71 ms /    19 runs   (  232.14 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:       total time =   35319.75 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     136.05 ms /    54 runs   (    2.52 ms per token,   396.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33473.93 ms /   154 tokens (  217.36 ms per token,     4.60 tokens per second)\n",
      "llama_print_timings:        eval time =   14903.04 ms /    53 runs   (  281.19 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =   50348.32 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      22.59 ms /    79 runs   (    0.29 ms per token,  3496.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   98612.83 ms /   795 tokens (  124.04 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time =   19253.41 ms /    78 runs   (  246.84 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:       total time =  118583.80 ms /   873 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =       9.98 ms /    34 runs   (    0.29 ms per token,  3406.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  102274.20 ms /   773 tokens (  132.31 ms per token,     7.56 tokens per second)\n",
      "llama_print_timings:        eval time =    8034.76 ms /    33 runs   (  243.48 ms per token,     4.11 tokens per second)\n",
      "llama_print_timings:       total time =  110776.86 ms /   806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     278.20 ms /   130 runs   (    2.14 ms per token,   467.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  111452.45 ms /   724 tokens (  153.94 ms per token,     6.50 tokens per second)\n",
      "llama_print_timings:        eval time =   37779.31 ms /   129 runs   (  292.86 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =  154748.14 ms /   853 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    18 runs   (    0.29 ms per token,  3395.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  116893.47 ms /   984 tokens (  118.79 ms per token,     8.42 tokens per second)\n",
      "llama_print_timings:        eval time =    4464.35 ms /    18 runs   (  248.02 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =  121816.13 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /    49 runs   (    0.29 ms per token,  3396.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  119933.98 ms /   954 tokens (  125.72 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time =   11969.44 ms /    48 runs   (  249.36 ms per token,     4.01 tokens per second)\n",
      "llama_print_timings:       total time =  132543.98 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =      49.79 ms /    23 runs   (    2.16 ms per token,   461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  135341.02 ms /   884 tokens (  153.10 ms per token,     6.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6380.54 ms /    22 runs   (  290.02 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =  144717.31 ms /   906 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      65.02 ms /   234 runs   (    0.28 ms per token,  3598.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45493.34 ms /   322 tokens (  141.28 ms per token,     7.08 tokens per second)\n",
      "llama_print_timings:        eval time =   55566.44 ms /   233 runs   (  238.48 ms per token,     4.19 tokens per second)\n",
      "llama_print_timings:       total time =  102344.92 ms /   555 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      14.62 ms /    48 runs   (    0.30 ms per token,  3283.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49398.00 ms /   319 tokens (  154.85 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:        eval time =   11234.79 ms /    47 runs   (  239.04 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =   61027.08 ms /   366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     131.80 ms /    66 runs   (    2.00 ms per token,   500.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55449.69 ms /   304 tokens (  182.40 ms per token,     5.48 tokens per second)\n",
      "llama_print_timings:        eval time =   18401.50 ms /    65 runs   (  283.10 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =   76497.86 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      17.24 ms /    47 runs   (    0.37 ms per token,  2726.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73220.02 ms /   576 tokens (  127.12 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time =   10933.27 ms /    46 runs   (  237.68 ms per token,     4.21 tokens per second)\n",
      "llama_print_timings:       total time =   84659.87 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      47.13 ms /   127 runs   (    0.37 ms per token,  2694.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73993.08 ms /   557 tokens (  132.84 ms per token,     7.53 tokens per second)\n",
      "llama_print_timings:        eval time =   29819.26 ms /   126 runs   (  236.66 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =  104694.84 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     124.31 ms /    46 runs   (    2.70 ms per token,   370.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   86582.76 ms /   538 tokens (  160.93 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:        eval time =   12693.25 ms /    45 runs   (  282.07 ms per token,     3.55 tokens per second)\n",
      "llama_print_timings:       total time =  102301.98 ms /   583 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      10.22 ms /    38 runs   (    0.27 ms per token,  3716.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  117533.02 ms /   965 tokens (  121.80 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8925.63 ms /    37 runs   (  241.23 ms per token,     4.15 tokens per second)\n",
      "llama_print_timings:       total time =  127046.43 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      28.86 ms /   100 runs   (    0.29 ms per token,  3465.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  114296.42 ms /   903 tokens (  126.57 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time =   24081.78 ms /    99 runs   (  243.25 ms per token,     4.11 tokens per second)\n",
      "llama_print_timings:       total time =  139289.34 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     122.66 ms /    59 runs   (    2.08 ms per token,   481.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  133152.14 ms /   837 tokens (  159.08 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:        eval time =   17016.46 ms /    58 runs   (  293.39 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:       total time =  154138.12 ms /   895 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      19.45 ms /    56 runs   (    0.35 ms per token,  2879.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =  110286.56 ms /   888 tokens (  124.20 ms per token,     8.05 tokens per second)\n",
      "llama_print_timings:        eval time =   13232.39 ms /    55 runs   (  240.59 ms per token,     4.16 tokens per second)\n",
      "llama_print_timings:       total time =  124139.52 ms /   943 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =       7.44 ms /    20 runs   (    0.37 ms per token,  2689.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  106494.89 ms /   847 tokens (  125.73 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time =    4602.25 ms /    19 runs   (  242.22 ms per token,     4.13 tokens per second)\n",
      "llama_print_timings:       total time =  111539.82 ms /   866 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     217.57 ms /    76 runs   (    2.86 ms per token,   349.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  120550.93 ms /   790 tokens (  152.60 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:        eval time =   21858.79 ms /    75 runs   (  291.45 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =  146584.61 ms /   865 tokens\n"
     ]
    }
   ],
   "source": [
    "for query, context in zip(QUERIES, CONTEXTS):\n",
    "\n",
    "    # llama\n",
    "    init_time = time.time()\n",
    "    answer = llama.get_answer(context, query)\n",
    "    total_time = time.time()-init_time\n",
    "    llama_metrics[\"words_per_second\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split())/total_time)\n",
    "    llama_metrics[\"words\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split()))\n",
    "\n",
    "    # mistral\n",
    "    init_time = time.time()\n",
    "    answer = mistral.get_answer(context, query)\n",
    "    total_time = time.time()-init_time\n",
    "    mistral_metrics[\"words_per_second\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split())/total_time)\n",
    "    mistral_metrics[\"words\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split()))\n",
    "\n",
    "    # gemma\n",
    "    init_time = time.time()\n",
    "    answer = gemma.get_answer(context, query)\n",
    "    total_time = time.time()-init_time\n",
    "    gemma_metrics[\"words_per_second\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split())/total_time)\n",
    "    gemma_metrics[\"words\"].append(len(re.sub(\"[^a-zA-Z']+\", ' ', answer).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral 7B vs Llama 2 7B vs Gemma 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_metrics = pd.DataFrame(mistral_metrics)\n",
    "mistral_metrics['model'] = 'Mistral 7B'\n",
    "llama_metrics = pd.DataFrame(llama_metrics)\n",
    "llama_metrics['model'] = 'Llama 2 7B'\n",
    "gemma_metrics = pd.DataFrame(gemma_metrics)\n",
    "gemma_metrics['model'] = 'Gemma 7B'\n",
    "\n",
    "# create single data frame for plotting\n",
    "metrics = pd.concat([mistral_metrics, llama_metrics, gemma_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Mistral 7B vs LLama 2 7B vs Gemma 7B')\n",
    "\n",
    "sns.barplot(ax=axes[0], data=metrics, y='words_per_second', x='model', hue='model', palette=[\"#dd4fe4\", \"#070620\", \"#fa7302\"])\n",
    "axes[0].set_title(\"Words per second\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=metrics, y='words', x='model', hue='model', palette=[\"#dd4fe4\", \"#070620\", \"#fa7302\"])\n",
    "axes[1].set_title(\"Avg Answer length\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 7B answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11491.93 ms\n",
      "llama_print_timings:      sample time =      22.98 ms /    65 runs   (    0.35 ms per token,  2828.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19831.71 ms /   101 tokens (  196.35 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:        eval time =   15040.72 ms /    64 runs   (  235.01 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =   35279.66 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " People like the product because it is easy to use, the screen is easy to read, the battery life is long, and they can use it for travel and at the beach. Additionally, people like that the product is updated with newer versions and they enjoy using it even if they did not originally buy it for themselves.\n",
      "-------------\n",
      "LLama 2 7B answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   11342.70 ms\n",
      "llama_print_timings:      sample time =      39.54 ms /   108 runs   (    0.37 ms per token,  2731.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21544.14 ms /   103 tokens (  209.17 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:        eval time =   24405.79 ms /   107 runs   (  228.09 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:       total time =   46559.87 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) It's a great way to read books on a device instead of carrying around a physical copy.\n",
      "        2) The battery life is long so you can read for hours without having to worry about charging.\n",
      "        3) The screen is easy to read even in bright sunlight or low light conditions.\n",
      "        4) The device is lightweight and portable so you can take it with you wherever you go.\n",
      "        5) It's easy to navigate through books on the device using its interface.\n",
      "Gemma 7B answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14468.73 ms\n",
      "llama_print_timings:      sample time =     154.75 ms /    54 runs   (    2.87 ms per token,   348.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27098.20 ms /   100 tokens (  270.98 ms per token,     3.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15291.33 ms /    53 runs   (  288.52 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   44315.41 ms /   153 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is your Answer:\n",
      "\n",
      "People like about  the  product its ease-of-use , readability , battery life , compact size , versatility ,  nice design ,  nice reading experience ,  and its ability  to be used in various settings .\n"
     ]
    }
   ],
   "source": [
    "print(\"Mistral 7B answer:\")\n",
    "print(mistral.get_answer(CONTEXTS[1], QUERIES[1]))\n",
    "print(\"-------------\")\n",
    "\n",
    "print(\"LLama 2 7B answer:\")\n",
    "print(llama.get_answer(CONTEXTS[1], QUERIES[1]))\n",
    "\n",
    "print(\"Gemma 7B answer:\")\n",
    "print(gemma.get_answer(CONTEXTS[1], QUERIES[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zaai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
