{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral vs LLama 2 vs Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafael/miniconda3/envs/zaai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = ..\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.04 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': '..', 'llama.vocab_size': '128256'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from /Users/rafael/Documents/lab/large-language-models/gemma/model/gemma-7b-it-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   28 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_rot            = 192\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.77 GiB (4.80 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4883.94 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.13 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     7.91 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '15', 'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'gemma', 'gemma.attention.head_count_kv': '16', 'gemma.feed_forward_length': '24576', 'tokenizer.ggml.bos_token_id': '2', 'gemma.embedding_length': '3072', 'gemma.block_count': '28', 'tokenizer.ggml.unknown_token_id': '3', 'gemma.attention.key_length': '256', 'gemma.context_length': '8192', 'general.name': 'gemma-7b-it', 'gemma.attention.value_length': '256', 'gemma.attention.head_count': '16'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from generator.generator import Generator\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "\n",
    "llama = Generator(model='llama')\n",
    "mistral = Generator(model='mistral')\n",
    "gemma = Generator(model='gemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Q&A Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train\")\n",
    "squad = squad.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary to save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": [],\n",
    "}\n",
    "\n",
    "mistral_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": [],\n",
    "}\n",
    "\n",
    "gemma_metrics = {\n",
    "    \"words_per_second\": [],\n",
    "    \"words\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafael/miniconda3/envs/zaai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      55.82 ms /    42 runs   (    1.33 ms per token,   752.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60056.77 ms /   366 tokens (  164.09 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:        eval time =   11165.54 ms /    41 runs   (  272.33 ms per token,     3.67 tokens per second)\n",
      "llama_print_timings:       total time =   72205.35 ms /   407 tokens\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    18 runs   (    0.35 ms per token,  2864.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68235.76 ms /   421 tokens (  162.08 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4320.44 ms /    17 runs   (  254.14 ms per token,     3.93 tokens per second)\n",
      "llama_print_timings:       total time =   72761.63 ms /   438 tokens\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     377.03 ms /   137 runs   (    2.75 ms per token,   363.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66635.19 ms /   384 tokens (  173.53 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =   40249.82 ms /   136 runs   (  295.95 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:       total time =  111226.92 ms /   520 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      18.24 ms /    17 runs   (    1.07 ms per token,   932.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21460.98 ms /    87 tokens (  246.68 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:        eval time =    4264.04 ms /    16 runs   (  266.50 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:       total time =   26046.53 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       4.54 ms /    16 runs   (    0.28 ms per token,  3524.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19369.40 ms /    96 tokens (  201.76 ms per token,     4.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3625.70 ms /    15 runs   (  241.71 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =   23072.50 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =       8.31 ms /     4 runs   (    2.08 ms per token,   481.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22445.81 ms /    88 tokens (  255.07 ms per token,     3.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1151.30 ms /     4 runs   (  287.83 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   23925.65 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      99.51 ms /    72 runs   (    1.38 ms per token,   723.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21478.45 ms /    74 tokens (  290.25 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:        eval time =   18912.54 ms /    71 runs   (  266.37 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:       total time =   41419.83 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    16 runs   (    0.33 ms per token,  3051.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17702.00 ms /    80 tokens (  221.28 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3220.80 ms /    15 runs   (  214.72 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =   21004.36 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     205.67 ms /    81 runs   (    2.54 ms per token,   393.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20637.12 ms /    78 tokens (  264.58 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:        eval time =   23200.81 ms /    80 runs   (  290.01 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =   46125.88 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      53.88 ms /    54 runs   (    1.00 ms per token,  1002.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30383.17 ms /   168 tokens (  180.85 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:        eval time =   13700.04 ms /    53 runs   (  258.49 ms per token,     3.87 tokens per second)\n",
      "llama_print_timings:       total time =   44947.49 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /    10 runs   (    0.29 ms per token,  3424.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35443.63 ms /   200 tokens (  177.22 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2234.68 ms /     9 runs   (  248.30 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =   37783.39 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =    1897.88 ms /   826 runs   (    2.30 ms per token,   435.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35263.51 ms /   176 tokens (  200.36 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:        eval time =  361759.23 ms /   826 runs   (  437.97 ms per token,     2.28 tokens per second)\n",
      "llama_print_timings:       total time =  421803.12 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      77.92 ms /    49 runs   (    1.59 ms per token,   628.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66446.37 ms /   223 tokens (  297.97 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:        eval time =   25882.88 ms /    48 runs   (  539.23 ms per token,     1.85 tokens per second)\n",
      "llama_print_timings:       total time =   93627.54 ms /   271 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =      14.17 ms /    34 runs   (    0.42 ms per token,  2399.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59824.21 ms /   268 tokens (  223.22 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:        eval time =   14221.26 ms /    33 runs   (  430.95 ms per token,     2.32 tokens per second)\n",
      "llama_print_timings:       total time =   74361.84 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     101.60 ms /    30 runs   (    3.39 ms per token,   295.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68319.14 ms /   242 tokens (  282.31 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:        eval time =   17835.04 ms /    29 runs   (  615.00 ms per token,     1.63 tokens per second)\n",
      "llama_print_timings:       total time =   88096.88 ms /   271 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     217.80 ms /   177 runs   (    1.23 ms per token,   812.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50418.97 ms /   174 tokens (  289.76 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:        eval time =   80737.32 ms /   176 runs   (  458.73 ms per token,     2.18 tokens per second)\n",
      "llama_print_timings:       total time =  134366.97 ms /   350 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       7.45 ms /    20 runs   (    0.37 ms per token,  2684.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51177.95 ms /   197 tokens (  259.79 ms per token,     3.85 tokens per second)\n",
      "llama_print_timings:        eval time =    7688.82 ms /    19 runs   (  404.67 ms per token,     2.47 tokens per second)\n",
      "llama_print_timings:       total time =   59064.58 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     191.90 ms /    79 runs   (    2.43 ms per token,   411.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48154.14 ms /   170 tokens (  283.26 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:        eval time =   39963.51 ms /    78 runs   (  512.35 ms per token,     1.95 tokens per second)\n",
      "llama_print_timings:       total time =   91364.59 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      66.93 ms /    53 runs   (    1.26 ms per token,   791.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45276.53 ms /   190 tokens (  238.30 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:        eval time =   26406.11 ms /    52 runs   (  507.81 ms per token,     1.97 tokens per second)\n",
      "llama_print_timings:       total time =   72815.81 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.48 ms /    12 runs   (    0.46 ms per token,  2189.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54390.30 ms /   216 tokens (  251.81 ms per token,     3.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4843.01 ms /    12 runs   (  403.58 ms per token,     2.48 tokens per second)\n",
      "llama_print_timings:       total time =   59411.80 ms /   228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      73.35 ms /    27 runs   (    2.72 ms per token,   368.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41249.92 ms /   184 tokens (  224.18 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:        eval time =    8795.34 ms /    27 runs   (  325.75 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =   51242.89 ms /   211 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      15.50 ms /    15 runs   (    1.03 ms per token,   967.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22084.10 ms /   102 tokens (  216.51 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time =    3965.37 ms /    14 runs   (  283.24 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =   26429.10 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /     5 runs   (    0.27 ms per token,  3654.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22534.55 ms /   116 tokens (  194.26 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:        eval time =     978.84 ms /     4 runs   (  244.71 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:       total time =   23592.85 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     172.35 ms /    85 runs   (    2.03 ms per token,   493.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25026.04 ms /    99 tokens (  252.79 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:        eval time =   27768.77 ms /    84 runs   (  330.58 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   55542.82 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     155.10 ms /   119 runs   (    1.30 ms per token,   767.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35237.14 ms /   200 tokens (  176.19 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =   33573.01 ms /   119 runs   (  282.13 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:       total time =   70858.38 ms /   319 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    15 runs   (    0.33 ms per token,  3052.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35278.38 ms /   232 tokens (  152.06 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:        eval time =    3772.84 ms /    14 runs   (  269.49 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:       total time =   39194.77 ms /   246 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      42.84 ms /    17 runs   (    2.52 ms per token,   396.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36804.78 ms /   203 tokens (  181.30 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4959.28 ms /    16 runs   (  309.95 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   42736.62 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      82.17 ms /    82 runs   (    1.00 ms per token,   997.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25390.09 ms /   138 tokens (  183.99 ms per token,     5.44 tokens per second)\n",
      "llama_print_timings:        eval time =   22676.67 ms /    81 runs   (  279.96 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =   49398.67 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       8.36 ms /    31 runs   (    0.27 ms per token,  3708.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29607.24 ms /   162 tokens (  182.76 ms per token,     5.47 tokens per second)\n",
      "llama_print_timings:        eval time =    7342.03 ms /    30 runs   (  244.73 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:       total time =   37140.68 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     154.79 ms /    53 runs   (    2.92 ms per token,   342.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28296.42 ms /   128 tokens (  221.07 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:        eval time =   17323.49 ms /    53 runs   (  326.86 ms per token,     3.06 tokens per second)\n",
      "llama_print_timings:       total time =   47379.00 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      20.62 ms /    20 runs   (    1.03 ms per token,   969.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38866.82 ms /   244 tokens (  159.29 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:        eval time =    5068.63 ms /    19 runs   (  266.77 ms per token,     3.75 tokens per second)\n",
      "llama_print_timings:       total time =   44563.76 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.65 ms /    24 runs   (    0.28 ms per token,  3608.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43951.04 ms /   288 tokens (  152.61 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:        eval time =    6131.29 ms /    24 runs   (  255.47 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:       total time =   50304.66 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =    1472.99 ms /   741 runs   (    1.99 ms per token,   503.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47128.08 ms /   261 tokens (  180.57 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:        eval time =  231250.18 ms /   740 runs   (  312.50 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time =  300960.96 ms /  1001 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      35.26 ms /    36 runs   (    0.98 ms per token,  1020.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28568.51 ms /   160 tokens (  178.55 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9127.04 ms /    36 runs   (  253.53 ms per token,     3.94 tokens per second)\n",
      "llama_print_timings:       total time =   38360.66 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =      11.33 ms /    43 runs   (    0.26 ms per token,  3796.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31728.03 ms /   200 tokens (  158.64 ms per token,     6.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9919.71 ms /    42 runs   (  236.18 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =   41895.03 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      70.50 ms /    36 runs   (    1.96 ms per token,   510.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30245.10 ms /   163 tokens (  185.55 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:        eval time =   10050.42 ms /    35 runs   (  287.15 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:       total time =   41681.66 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     800.96 ms /   796 runs   (    1.01 ms per token,   993.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32402.11 ms /   208 tokens (  155.78 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:        eval time =  219180.79 ms /   796 runs   (  275.35 ms per token,     3.63 tokens per second)\n",
      "llama_print_timings:       total time =  262673.75 ms /  1004 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    21 runs   (    0.29 ms per token,  3393.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35405.32 ms /   243 tokens (  145.70 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:        eval time =    6868.28 ms /    20 runs   (  343.41 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:       total time =   42465.38 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     108.31 ms /    52 runs   (    2.08 ms per token,   480.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48588.81 ms /   222 tokens (  218.87 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:        eval time =   17522.51 ms /    51 runs   (  343.58 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:       total time =   68327.31 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      28.99 ms /    21 runs   (    1.38 ms per token,   724.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33960.80 ms /   167 tokens (  203.36 ms per token,     4.92 tokens per second)\n",
      "llama_print_timings:        eval time =    8193.75 ms /    20 runs   (  409.69 ms per token,     2.44 tokens per second)\n",
      "llama_print_timings:       total time =   42752.65 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    17 runs   (    0.34 ms per token,  2978.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52522.40 ms /   208 tokens (  252.51 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5467.82 ms /    16 runs   (  341.74 ms per token,     2.93 tokens per second)\n",
      "llama_print_timings:       total time =   58177.83 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     130.81 ms /    65 runs   (    2.01 ms per token,   496.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37937.83 ms /   170 tokens (  223.16 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:        eval time =   19157.68 ms /    64 runs   (  299.34 ms per token,     3.34 tokens per second)\n",
      "llama_print_timings:       total time =   59488.45 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      86.85 ms /    64 runs   (    1.36 ms per token,   736.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27741.36 ms /   134 tokens (  207.03 ms per token,     4.83 tokens per second)\n",
      "llama_print_timings:        eval time =   16975.93 ms /    63 runs   (  269.46 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:       total time =   45914.74 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    15 runs   (    0.40 ms per token,  2489.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28414.57 ms /   154 tokens (  184.51 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:        eval time =    3500.17 ms /    14 runs   (  250.01 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:       total time =   32058.70 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     122.85 ms /    46 runs   (    2.67 ms per token,   374.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27864.17 ms /   141 tokens (  197.62 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:        eval time =   12754.41 ms /    45 runs   (  283.43 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =   42377.72 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     104.52 ms /    95 runs   (    1.10 ms per token,   908.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25682.55 ms /   155 tokens (  165.69 ms per token,     6.04 tokens per second)\n",
      "llama_print_timings:        eval time =   24168.82 ms /    94 runs   (  257.12 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =   51444.08 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.14 ms /    17 runs   (    0.30 ms per token,  3308.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27356.56 ms /   175 tokens (  156.32 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:        eval time =    3708.60 ms /    16 runs   (  231.79 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:       total time =   31219.98 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      61.25 ms /    28 runs   (    2.19 ms per token,   457.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34618.63 ms /   162 tokens (  213.70 ms per token,     4.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.68 ms /    27 runs   (  352.32 ms per token,     2.84 tokens per second)\n",
      "llama_print_timings:       total time =   45364.22 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      23.96 ms /    20 runs   (    1.20 ms per token,   834.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29027.62 ms /   136 tokens (  213.44 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5291.14 ms /    19 runs   (  278.48 ms per token,     3.59 tokens per second)\n",
      "llama_print_timings:       total time =   34816.88 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    16 runs   (    0.41 ms per token,  2454.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29811.37 ms /   159 tokens (  187.49 ms per token,     5.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3722.84 ms /    15 runs   (  248.19 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =   33681.89 ms /   174 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      72.23 ms /    25 runs   (    2.89 ms per token,   346.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31070.24 ms /   136 tokens (  228.46 ms per token,     4.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9703.66 ms /    25 runs   (  388.15 ms per token,     2.58 tokens per second)\n",
      "llama_print_timings:       total time =   41891.05 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      88.14 ms /    83 runs   (    1.06 ms per token,   941.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39100.60 ms /   196 tokens (  199.49 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:        eval time =   21791.14 ms /    82 runs   (  265.75 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =   62355.15 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    22 runs   (    0.29 ms per token,  3392.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36872.12 ms /   216 tokens (  170.70 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5167.53 ms /    22 runs   (  234.89 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =   42241.30 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     103.90 ms /    38 runs   (    2.73 ms per token,   365.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39754.53 ms /   199 tokens (  199.77 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:        eval time =   10681.18 ms /    37 runs   (  288.68 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   51985.58 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      56.38 ms /    43 runs   (    1.31 ms per token,   762.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30967.15 ms /   152 tokens (  203.73 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:        eval time =   10782.48 ms /    42 runs   (  256.73 ms per token,     3.90 tokens per second)\n",
      "llama_print_timings:       total time =   42550.14 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       4.98 ms /    14 runs   (    0.36 ms per token,  2811.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32996.71 ms /   178 tokens (  185.37 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:        eval time =    3045.03 ms /    13 runs   (  234.23 ms per token,     4.27 tokens per second)\n",
      "llama_print_timings:       total time =   36170.90 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      39.51 ms /    15 runs   (    2.63 ms per token,   379.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36945.35 ms /   164 tokens (  225.28 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:        eval time =    3985.86 ms /    14 runs   (  284.70 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   41787.13 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =    1323.31 ms /   934 runs   (    1.42 ms per token,   705.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21485.55 ms /    71 tokens (  302.61 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:        eval time =  265552.40 ms /   933 runs   (  284.62 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =  303273.03 ms /  1004 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       9.35 ms /    24 runs   (    0.39 ms per token,  2567.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25982.20 ms /    84 tokens (  309.31 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6209.27 ms /    23 runs   (  269.97 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =   32384.59 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     113.67 ms /    39 runs   (    2.91 ms per token,   343.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31572.50 ms /    83 tokens (  380.39 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:        eval time =   12846.96 ms /    38 runs   (  338.08 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =   46147.60 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      56.47 ms /    49 runs   (    1.15 ms per token,   867.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42823.72 ms /   159 tokens (  269.33 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:        eval time =   14033.57 ms /    48 runs   (  292.37 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =   57927.81 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    12 runs   (    0.32 ms per token,  3088.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43006.90 ms /   178 tokens (  241.61 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2748.46 ms /    11 runs   (  249.86 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:       total time =   45901.96 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      99.47 ms /    45 runs   (    2.21 ms per token,   452.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38046.24 ms /   154 tokens (  247.05 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:        eval time =   12733.97 ms /    44 runs   (  289.41 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   52688.91 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      97.15 ms /    64 runs   (    1.52 ms per token,   658.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56866.82 ms /   246 tokens (  231.17 ms per token,     4.33 tokens per second)\n",
      "llama_print_timings:        eval time =   21584.44 ms /    63 runs   (  342.61 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:       total time =   80336.23 ms /   309 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /    18 runs   (    0.43 ms per token,  2328.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57450.78 ms /   276 tokens (  208.15 ms per token,     4.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4854.66 ms /    17 runs   (  285.57 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =   62592.41 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      63.31 ms /    25 runs   (    2.53 ms per token,   394.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60974.75 ms /   255 tokens (  239.12 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:        eval time =    7868.15 ms /    24 runs   (  327.84 ms per token,     3.05 tokens per second)\n",
      "llama_print_timings:       total time =   70634.98 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      10.76 ms /     9 runs   (    1.20 ms per token,   836.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38257.03 ms /   187 tokens (  204.58 ms per token,     4.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2067.00 ms /     8 runs   (  258.37 ms per token,     3.87 tokens per second)\n",
      "llama_print_timings:       total time =   40760.76 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    16 runs   (    0.41 ms per token,  2464.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41424.06 ms /   218 tokens (  190.02 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3596.11 ms /    15 runs   (  239.74 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:       total time =   45207.02 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     205.41 ms /    88 runs   (    2.33 ms per token,   428.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49259.08 ms /   184 tokens (  267.71 ms per token,     3.74 tokens per second)\n",
      "llama_print_timings:        eval time =   27739.90 ms /    88 runs   (  315.23 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:       total time =   80498.87 ms /   272 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     959.50 ms /   795 runs   (    1.21 ms per token,   828.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45613.91 ms /   210 tokens (  217.21 ms per token,     4.60 tokens per second)\n",
      "llama_print_timings:        eval time =  218165.44 ms /   794 runs   (  274.77 ms per token,     3.64 tokens per second)\n",
      "llama_print_timings:       total time =  277919.02 ms /  1004 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    13 runs   (    0.35 ms per token,  2841.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42975.69 ms /   247 tokens (  173.99 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =    2906.30 ms /    12 runs   (  242.19 ms per token,     4.13 tokens per second)\n",
      "llama_print_timings:       total time =   46068.74 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      42.30 ms /    18 runs   (    2.35 ms per token,   425.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47439.51 ms /   220 tokens (  215.63 ms per token,     4.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5082.53 ms /    17 runs   (  298.97 ms per token,     3.34 tokens per second)\n",
      "llama_print_timings:       total time =   53722.21 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      79.46 ms /    71 runs   (    1.12 ms per token,   893.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79507.92 ms /   498 tokens (  159.65 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:        eval time =   18114.63 ms /    70 runs   (  258.78 ms per token,     3.86 tokens per second)\n",
      "llama_print_timings:       total time =   99428.97 ms /   568 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =      20.76 ms /    66 runs   (    0.31 ms per token,  3179.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80720.39 ms /   550 tokens (  146.76 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:        eval time =   17124.87 ms /    65 runs   (  263.46 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:       total time =   98391.61 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     102.18 ms /    34 runs   (    3.01 ms per token,   332.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   94942.72 ms /   490 tokens (  193.76 ms per token,     5.16 tokens per second)\n",
      "llama_print_timings:        eval time =   10894.94 ms /    33 runs   (  330.15 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =  108648.40 ms /   523 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     308.60 ms /   205 runs   (    1.51 ms per token,   664.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26519.40 ms /   120 tokens (  220.99 ms per token,     4.52 tokens per second)\n",
      "llama_print_timings:        eval time =   55797.59 ms /   205 runs   (  272.18 ms per token,     3.67 tokens per second)\n",
      "llama_print_timings:       total time =   85973.84 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       9.25 ms /    23 runs   (    0.40 ms per token,  2487.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31240.27 ms /   142 tokens (  220.00 ms per token,     4.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5355.42 ms /    22 runs   (  243.43 ms per token,     4.11 tokens per second)\n",
      "llama_print_timings:       total time =   36899.27 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     139.78 ms /    49 runs   (    2.85 ms per token,   350.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32608.37 ms /   120 tokens (  271.74 ms per token,     3.68 tokens per second)\n",
      "llama_print_timings:        eval time =   14410.35 ms /    49 runs   (  294.09 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time =   49061.43 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     562.78 ms /   390 runs   (    1.44 ms per token,   692.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59382.10 ms /   315 tokens (  188.51 ms per token,     5.30 tokens per second)\n",
      "llama_print_timings:        eval time =  110197.59 ms /   389 runs   (  283.28 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =  176508.55 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /    13 runs   (    0.42 ms per token,  2400.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54801.53 ms /   343 tokens (  159.77 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3039.81 ms /    12 runs   (  253.32 ms per token,     3.95 tokens per second)\n",
      "llama_print_timings:       total time =   58050.73 ms /   355 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      70.70 ms /    21 runs   (    3.37 ms per token,   297.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61257.86 ms /   317 tokens (  193.24 ms per token,     5.17 tokens per second)\n",
      "llama_print_timings:        eval time =    7518.60 ms /    20 runs   (  375.93 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =   70512.10 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     115.89 ms /    87 runs   (    1.33 ms per token,   750.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40320.18 ms /   166 tokens (  242.89 ms per token,     4.12 tokens per second)\n",
      "llama_print_timings:        eval time =   29047.54 ms /    86 runs   (  337.76 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =   71223.16 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =      31.18 ms /   101 runs   (    0.31 ms per token,  3239.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39366.06 ms /   186 tokens (  211.65 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:        eval time =   25423.62 ms /   100 runs   (  254.24 ms per token,     3.93 tokens per second)\n",
      "llama_print_timings:       total time =   65477.28 ms /   286 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     202.76 ms /    87 runs   (    2.33 ms per token,   429.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43452.10 ms /   183 tokens (  237.44 ms per token,     4.21 tokens per second)\n",
      "llama_print_timings:        eval time =   26197.55 ms /    86 runs   (  304.62 ms per token,     3.28 tokens per second)\n",
      "llama_print_timings:       total time =   73092.13 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =      26.37 ms /    24 runs   (    1.10 ms per token,   909.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32670.87 ms /   141 tokens (  231.71 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:        eval time =    6641.37 ms /    23 runs   (  288.76 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   40137.16 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       5.84 ms /    15 runs   (    0.39 ms per token,  2567.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33146.19 ms /   160 tokens (  207.16 ms per token,     4.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3592.22 ms /    15 runs   (  239.48 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =   36898.63 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =     128.15 ms /    46 runs   (    2.79 ms per token,   358.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34551.33 ms /   150 tokens (  230.34 ms per token,     4.34 tokens per second)\n",
      "llama_print_timings:        eval time =   12445.18 ms /    45 runs   (  276.56 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   48973.04 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11356.44 ms\n",
      "llama_print_timings:      sample time =     848.37 ms /   739 runs   (    1.15 ms per token,   871.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36797.01 ms /   188 tokens (  195.73 ms per token,     5.11 tokens per second)\n",
      "llama_print_timings:        eval time =  198753.48 ms /   738 runs   (  269.31 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:       total time =  248570.73 ms /   926 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11236.46 ms\n",
      "llama_print_timings:      sample time =       1.53 ms /     4 runs   (    0.38 ms per token,  2619.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40325.49 ms /   203 tokens (  198.65 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:        eval time =     723.20 ms /     3 runs   (  241.07 ms per token,     4.15 tokens per second)\n",
      "llama_print_timings:       total time =   41165.62 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14147.44 ms\n",
      "llama_print_timings:      sample time =      79.34 ms /    34 runs   (    2.33 ms per token,   428.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43186.68 ms /   196 tokens (  220.34 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.83 ms /    33 runs   (  287.84 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   54268.82 ms /   229 tokens\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    context = squad[i]['context']\n",
    "    query = squad[i]['question']\n",
    "    answer = squad[i]['answers']['text'][0]\n",
    "\n",
    "    # llama\n",
    "    answer_llama, words_per_second, words = utils.get_llm_response(llama, context, query)\n",
    "    llama_metrics[\"words_per_second\"].append(words_per_second)\n",
    "    llama_metrics[\"words\"].append(words)\n",
    "\n",
    "    # mistral\n",
    "    answer_mistral, words_per_second, words = utils.get_llm_response(mistral, context, query)\n",
    "    mistral_metrics[\"words_per_second\"].append(words_per_second)\n",
    "    mistral_metrics[\"words\"].append(words)\n",
    "\n",
    "    # gemma\n",
    "    answer_gemma, words_per_second, words = utils.get_llm_response(gemma, context, query)\n",
    "    gemma_metrics[\"words_per_second\"].append(words_per_second)\n",
    "    gemma_metrics[\"words\"].append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral 7B vs Llama 2 7B vs Gemma 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_metrics = pd.DataFrame(mistral_metrics)\n",
    "mistral_metrics['model'] = 'Mistral 7B'\n",
    "llama_metrics = pd.DataFrame(llama_metrics)\n",
    "llama_metrics['model'] = 'Llama 3 8B'\n",
    "gemma_metrics = pd.DataFrame(gemma_metrics)\n",
    "gemma_metrics['model'] = 'Gemma 7B'\n",
    "\n",
    "# create single data frame for plotting\n",
    "metrics = pd.concat([mistral_metrics, llama_metrics, gemma_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Mistral 7B vs LLama 3 8B vs Gemma 7B')\n",
    "\n",
    "sns.barplot(ax=axes[0], data=metrics, y='words_per_second', x='model', hue='model', palette=[\"#dd4fe4\", \"#070620\", \"#fa7302\"])\n",
    "axes[0].set_title(\"Words per second\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=metrics, y='words', x='model', hue='model', palette=[\"#dd4fe4\", \"#070620\", \"#fa7302\"])\n",
    "axes[1].set_title(\"Avg Answer length\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zaai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
